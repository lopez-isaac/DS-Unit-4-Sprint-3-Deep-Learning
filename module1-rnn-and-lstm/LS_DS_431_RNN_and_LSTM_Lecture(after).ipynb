{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Lesson 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "## _aka_ PREDICTING THE FUTURE!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[2467])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0127 11:41:28.526748 4769819968 deprecation.py:323] From /Users/jonathansokoll/anaconda3/envs/U4-S3-DNN/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 75s 3ms/sample - loss: 0.4608 - accuracy: 0.7830 - val_loss: 0.3750 - val_accuracy: 0.8365\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.3047 - accuracy: 0.8740 - val_loss: 0.3893 - val_accuracy: 0.8296\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.2173 - accuracy: 0.9155 - val_loss: 0.4214 - val_accuracy: 0.8329\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 71s 3ms/sample - loss: 0.1521 - accuracy: 0.9428 - val_loss: 0.4941 - val_accuracy: 0.8216\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.1162 - accuracy: 0.9576 - val_loss: 0.4938 - val_accuracy: 0.8240\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 78s 3ms/sample - loss: 0.0818 - accuracy: 0.9712 - val_loss: 0.6287 - val_accuracy: 0.8179\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.0590 - accuracy: 0.9794 - val_loss: 0.7587 - val_accuracy: 0.8145\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.0464 - accuracy: 0.9833 - val_loss: 0.7539 - val_accuracy: 0.8117\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.0382 - accuracy: 0.9878 - val_loss: 0.8128 - val_accuracy: 0.8183\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.0324 - accuracy: 0.9890 - val_loss: 0.9054 - val_accuracy: 0.8148\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 71s 3ms/sample - loss: 0.0198 - accuracy: 0.9932 - val_loss: 0.9458 - val_accuracy: 0.8124\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 82s 3ms/sample - loss: 0.0166 - accuracy: 0.9949 - val_loss: 1.0020 - val_accuracy: 0.8057\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 82s 3ms/sample - loss: 0.0135 - accuracy: 0.9957 - val_loss: 1.1265 - val_accuracy: 0.8134\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 82s 3ms/sample - loss: 0.0127 - accuracy: 0.9958 - val_loss: 1.0269 - val_accuracy: 0.8121\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 82s 3ms/sample - loss: 0.0104 - accuracy: 0.9969 - val_loss: 1.1666 - val_accuracy: 0.8123\n",
      "25000/25000 [==============================] - 23s 906us/sample - loss: 1.1666 - accuracy: 0.8123\n",
      "Test score: 1.166573968951106\n",
      "Test accuracy: 0.81228\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "### LSTM Text generation with Keras\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "text = \" \".join(data)\n",
    "\n",
    "chars = list(set(text))\n",
    "\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 178374\n"
     ]
    }
   ],
   "source": [
    "# Create the Sequence Data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 characters long\n",
    "next_chars = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences:', len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 121)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature=1.0)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178374 samples\n",
      "Epoch 1/20\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.4007\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"day.\n",
      "\n",
      "His behavior during a news confere\"\n",
      "day.\n",
      "\n",
      "His behavior during a news confere bot a stoy roucofor ber: promentidis Kresy te copwing te ther. Ho contiagh salge hes padas s foc rming” In Sabe ur uwh intomsunt Ht meblsel tul Fonteroc 2yvicth of bemunigas. Thing Spucing at essonderpes tovie Bmasced I Tpeonning ands ain thy vewnon per_end arperos aler anl cond ap suns farling Paube dicthisp los die. shous for daty big. Hast ffarand aw’ at wabe sest andtreg to (ly Nand foak slan\n",
      "178374/178374 [==============================] - 60s 335us/sample - loss: 2.4006\n",
      "Epoch 2/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.2864\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \" slightly different versions of the main\"\n",
      " slightly different versions of the main zarl's.” Chise ackorssmicicg fpercat the nively merings ant of Wrtalll sees at infarpalt, whigp arexan abpe that the us maboot foid us froch Cisi ingaw sentionfel eam a eI-203 1Nroay ar oed mouruc andern af the haud, chhengh Kabe thim suts nithed socuge bived MnDashed ib osis. wrime the cala the \n",
      "ACY\n",
      "\n",
      "Skease S.. “II bit tre — . Ws dcanbent or, the pertead’s\n",
      "\n",
      "DA Arlrar:\n",
      "\n",
      "“Datisalimenat of uspangeg\n",
      "178374/178374 [==============================] - 62s 346us/sample - loss: 2.2865\n",
      "Epoch 3/20\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.2097\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"ining traction, but it’ll be a long and \"\n",
      "ining traction, but it’ll be a long and owit ts ctrouid atreacovingr vesily ha fadpers. racesse than the parct quase thes me to che prisionte prot to dloblids. Peals duthabe thith esorge party to gop’s the giscicaisgtare got ang an shiont, invin insench seave tukk on mpovel. egeny.\n",
      "\n",
      "“The Uviad. “Dinge praat. whe moin leebans’ the conpeds -mants thbore in Twithtle gaup to sout speling ove rave, has tyte supte.\n",
      "\n",
      "ROmang the Aund and aldo a\n",
      "178374/178374 [==============================] - 60s 334us/sample - loss: 2.2098\n",
      "Epoch 4/20\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.1485\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \" to the Turks, he surely cited them to T\"\n",
      " to the Turks, he surely cited them to Teruad whe’l has dim sigeges it nows gas sexprages tave us the “Bes ceporpwr. Was onghiy a fiod Gorking evrimad a qury, thing busefichis see ended.\n",
      "\n",
      "Food Calllly parevesinT T.\n",
      "\n",
      "“Weingest ot trut Peroton Houms For intain Republe tho shice aspould, ““We a proting by a das peck. Be — an Verciaig tuma’t on Mncte, ame fatien cupacors, lase, TE catt hit devient suppouss sait.\n",
      "\n",
      "The eal and sine Vow by dro\n",
      "178374/178374 [==============================] - 61s 345us/sample - loss: 2.1485\n",
      "Epoch 5/20\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.0964\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"ole host of tangible gains as well as th\"\n",
      "ole host of tangible gains as well as that deisre dasuich o batidmes of a clomis, as dent in a fist to nof creagnty the lore in loury sake loon” and shissust that bus” to conders.\n",
      "\n",
      "The Wellin soble to filion ace asryily by themelical on nof id to leccnstion ferivonter ancaly shigh wathing or that of the $Reabike Thinly alowatha lacks smoteriant in yshire arserme, a, be Eushir, Melicingosman soust intalseds higher tell, at pensey movench\n",
      "178374/178374 [==============================] - 57s 320us/sample - loss: 2.0965\n",
      "Epoch 6/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.0506\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"a restaurant, do you tip the same percen\"\n",
      "a restaurant, do you tip the same percenon crisobountasity worrgrowss is appoors. Stame ander fullient has I MFquitser thes stause hod.\n",
      "\n",
      "In Peasead, The Lille and gote. Yunioled 1000 on-1974 and Thistafe Dast orxain at Sexvefages hurgs:\n",
      "\n",
      "Thut sany. 13. We forke colpentient stanie Dont, Heil kroughs fore resing.\n",
      "\n",
      "The West. If rvaces om dage to with into pensed for munded, trants campess the in., the is he fispear a camment.\n",
      "\n",
      "In reade est\n",
      "178374/178374 [==============================] - 58s 326us/sample - loss: 2.0507\n",
      "Epoch 7/20\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.0094\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"have reason to worry.\n",
      "\n",
      "AD\n",
      "\n",
      "2. Los Angele\"\n",
      "have reason to worry.\n",
      "\n",
      "AD\n",
      "\n",
      "2. Los Angelerbogel nemention on the “Ase7 that wlees to of the Untornes quilia spale tho you in chill of fon the cours I 10 and croming byo woich phe by versenclions,, poon foud one cousty “AT. Himagh ong Russon xiver ifnerfing the achede-conts now shathie a a duar sure, shherr Amer socod mome — wot nogh conlife, fur” beces langs and baces odmarts Welly’s was game on and rhingern, and in uh ad miet, the pamin\n",
      "178374/178374 [==============================] - 63s 351us/sample - loss: 2.0096\n",
      "Epoch 8/20\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 1.9715\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"g elbow surgery. Mason Rudolph, his repl\"\n",
      "g elbow surgery. Mason Rudolph, his replay igricaused and?\n",
      "\n",
      "AD\n",
      "\n",
      "Vike 12 a for at the’se arcearies Donts’ wile Hesy has mipted sheakies fos dusweit, pear gaing.\n",
      "\n",
      "Anger hand  was ississter — all stibles somemedwers in akeess of the 57-Haidw\n",
      "\n",
      "The, the govering towd wan Deason Pust Inchusiul, Thiterel Leinis Osteand), and as have assine scould they donged thimily on to eringed and mean Pensogee Stare Unitor bning indervergo ar or gare’s des\n",
      "178374/178374 [==============================] - 62s 346us/sample - loss: 1.9716\n",
      "Epoch 9/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.9377\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \"erman-held stone house, firing with an a\"\n",
      "erman-held stone house, firing with an ad off Chongent of the Masthen KFwhesh sandar suemety beitles may the RFWer and doveragies on lew herch or nlow lins do merrignd⁩hor appenional ingute eftense peoting bod toke wite in execuote convine geted and the wikn in Senten 101, wooker peaserthing a thams the ats sectomity, advectorized conting juscritile weratly?\n",
      "\n",
      "Ker worost incoutile sicle achanies hals twile it Zncarst musw you teonéred en\n",
      "178374/178374 [==============================] - 61s 343us/sample - loss: 1.9377\n",
      "Epoch 10/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.9062\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"ump impeachment inquiry, Starr added, “i\"\n",
      "ump impeachment inquiry, Starr added, “if Putys trear compoly that hempies fohe weres from coumn Rudut reading to spersed.\n",
      "\n",
      "My if like Tirkh in they spee in Many intual and aroust a treeting has vine o withns, the state. The may, The and she noug us s’re peaptes..\n",
      "Aht trate-fact with new harnedd? Werlate to werk the pates or Trupp duson’t to at vere wold to mo the wite way pay in load for a fuct. In the ressed the said in mide twe to sw\n",
      "178374/178374 [==============================] - 50s 280us/sample - loss: 1.9062\n",
      "Epoch 11/20\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 1.8777\n",
      "----- Generating text after Epoch: 10\n",
      "----- Generating with seed: \" August. It needs a quorum to function —\"\n",
      " August. It needs a quorum to function — Scub Lushinghons”.”\n",
      "\n",
      "AD\n",
      "\n",
      "A shat the supdaseman — maks, when DoC In I. Sttton the nerign wils may Ire wornd it whre like use.\n",
      "\n",
      "S. Daskits deichal erals the 2067. For colfted an zone to wilderinks arlical nemmized” frir-medning kin altorictive stape the cholorgly the has now his verdany a stitup trough, Declify Heighu sough Morce in Burred to Trump appless co rail.\n",
      "\n",
      "The sppoors of the hore, an long\n",
      "178374/178374 [==============================] - 49s 275us/sample - loss: 1.8776\n",
      "Epoch 12/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.8507\n",
      "----- Generating text after Epoch: 11\n",
      "----- Generating with seed: \",” and showed up to argue that burning d\"\n",
      ",” and showed up to argue that burning despension\n",
      "\n",
      "It Je'sand Pairanns, as the undiremy and and retersaned.\n",
      "\n",
      "NHe MaBda Sittool Fillice Anexicaler Caria on that an approccheling and Delicons and the sechimercestan stated doce atroing flockshiiv scriending as tree for “Burricaria Bury and scoude to 212 forts of WarrgB. RoAd For proency.’\n",
      "\n",
      "Hewster has envoor wasce, the Denchuss orating. But doows and hem hose to that the iscupated twat way\n",
      "178374/178374 [==============================] - 49s 274us/sample - loss: 1.8506\n",
      "Epoch 13/20\n",
      "  9600/178374 [>.............................] - ETA: 41s - loss: 1.8171"
     ]
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_gpus()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
