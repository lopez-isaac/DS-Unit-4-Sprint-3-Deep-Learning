{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# TODO - Words, words, mere words, no matter from the heart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "r = requests.get(url)\n",
    "r.encoding = r.apparent_encoding\n",
    "data = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget \n",
    "SS = wget.download(\"https://www.gutenberg.org/files/100/100-0.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100-0.txt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "with open('100-0.txt', 'r') as f:\n",
    "    data.append(f.read())\n",
    "        \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace html syntax with spaces\n",
    "text = data[0].replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep alphanumerical characters\n",
    "import re\n",
    "text = re.sub(r'[^a-zA-Z^0-9]', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5406250"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode data as characters\n",
    "chars = list(set(text))\n",
    "\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 1081214\n"
     ]
    }
   ],
   "source": [
    "# Create the Sequence Data\n",
    "\n",
    "maxlen = 180\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 180 characters long\n",
    "next_chars = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences2 = sequences[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Specify x & y\n",
    "\n",
    "x = np.zeros((len(sequences2), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences2), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences2):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 180, 106)\n",
      "(100000, 106)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature=1.0)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples\n",
      "Epoch 1/5\n",
      " 99900/100000 [============================>.] - ETA: 0s - loss: 2.7197\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"net, whom none resist.PERICLES.Antiochus, I thank thee, who hath taughtMy frail mortality to know itself,And by those fearful objects to prepareThis body, like to them, to what I m\"\n",
      "net, whom none resist.PERICLES.Antiochus, I thank thee, who hath taughtMy frail mortality to know itself,And by those fearful objects to prepareThis body, like to them, to what I mEe.  AO se I  LeT vias u  Coc  TeG cop’, h  l Horr nomf, an  S.  K Iun .  i   an   ci ,TShemeg t  aonpitn,  d Ae  -   Oisiro  Tfecdic   e doWhor bEpeitht  Re  o  I  agl  on    Ooln pou Rhai     Iome   hr bragi  I  fowke l O   R  ALH ThallesewoPphtsitilt mans hete d  l  t porrenMe C  E Gêamenyti     eu r th  hef  l ro s hoterilmenr siwe, f C. bi c  OPe mGare whplchutheilelaetdehashas tousgr.  ] and\n",
      "100000/100000 [==============================] - 175s 2ms/sample - loss: 2.7194\n",
      "Epoch 2/5\n",
      " 99900/100000 [============================>.] - ETA: 0s - loss: 2.4855\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"rom my griefs,And woes by wrong imaginations loseThe knowledge of themselves. [_A drum afar off._]EDGAR.Give me your hand.Far off methinks I hear the beaten drum.Come, father, I’ll\"\n",
      "rom my griefs,And woes by wrong imaginations loseThe knowledge of themselves. [_A drum afar off._]EDGAR.Give me your hand.Far off methinks I hear the beaten drum.Come, father, I’ll slotous s eouI  ifr!or our ucenbOfonesernrt,     avegscr k ne, This.  mWeft wourpnd he-s k.lind I ofeeshr He Woon neve  wouace, eiTchebuo wosHmorit. OMNROP. I tfe theatsesinmnknausonod 'ne domer  houcev, Ohememy sill larlPre  Thame liapef:h ma’e sove codewraen yorrrun, ie be-t| ttthero no d ther   an poro; breessan wdole :hesye-thapuveagerettind    tine hevinges, besdEONeer seishe noke morn;yor, \n",
      "100000/100000 [==============================] - 178s 2ms/sample - loss: 2.4854\n",
      "Epoch 3/5\n",
      " 99900/100000 [============================>.] - ETA: 0s - loss: 2.3738\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"nst that time do I ensconce me hereWithin the knowledge of mine own desert,And this my hand, against my self uprear,To guard the lawful reasons on thy part,  To leave poor me, thou\"\n",
      "nst that time do I ensconce me hereWithin the knowledge of mine own desert,And this my hand, against my self uprear,To guard the lawful reasons on thy part,  To leave poor me, thoufne d MPoI tour tont ond Andedrill oudut m. anit ne hecano sat. arthen 'A NoD. IMHNThar the war,ok this thod dint ros wos sond thonk, ther teve mrace vic’nd Ilumt, f un hokg,, s ine anen? I BROA. Lrechir  astrtind shet riplittine fy af; ther that orit  rover meth LLPARAMTAREALRRLNLA. Bals  CsoXsber.,I LELAI. D anco hes anwean xonownroucemi, wise.   CrG.TaBen ? OLLAA.SLo nam, beise,Atbwant drit sat\n",
      "100000/100000 [==============================] - 180s 2ms/sample - loss: 2.3739\n",
      "Epoch 4/5\n",
      " 99900/100000 [============================>.] - ETA: 0s - loss: 2.2973\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"gain I dare not.LADY MACBETH.Infirm of purpose!Give me the daggers. The sleeping and the deadAre but as pictures. ’Tis the eye of childhoodThat fears a painted devil. If he do blee\"\n",
      "gain I dare not.LADY MACBETH.Infirm of purpose!Give me the daggers. The sleeping and the deadAre but as pictures. ’Tis the eye of childhoodThat fears a painted devil. If he do blees shot womarsit  lo d petiswin  o  roag.  EN , Cofrert worofond f. spe. hals Thanmy he alphleingutse the nown or pomeator foone Pfond orC PROC.YEPAbler Il tht lel.   m n  fithen, enfend and you dacfna lnies for lhas nal  menge patr tor foomy hea limins sones mink bekdnce sthis  ofime th hengur, -thy ketThan in wheny do lact asth hers, ars! COSOYENA.Donde i shanometr Dpaenai'n;       of bustour kL_\n",
      "100000/100000 [==============================] - 180s 2ms/sample - loss: 2.2973\n",
      "Epoch 5/5\n",
      " 99900/100000 [============================>.] - ETA: 0s - loss: 2.2410\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"foulAs Vulcan’s stithy. Give him heedful note;For I mine eyes will rivet to his face;And after we will both our judgments joinIn censure of his seeming.HORATIO.Well, my lord.If he \"\n",
      "foulAs Vulcan’s stithy. Give him heedful note;For I mine eyes will rivet to his face;And after we will both our judgments joinIn censure of his seeming.HORATIO.Well, my lord.If he bast ftod t  f or mindeIw  fperinF nany me harigrd Annaygrs. Whaves beche  hole ly nNome urrirsprocs! AMOWMis tht the be; And us  novy I that ine thagy ware. Yo  fach papel sart cersetretnof that if mlangs, bthad,    Gove’n tonk AtR OUNOES. DOngothingur,wan  Want!    A I the hove, mer’s no giytreis’sm sen thand dors waml ding bet ins loslureas und cro, your.  nDshanden the bace.   S     En,      T\n",
      "100000/100000 [==============================] - 180s 2ms/sample - loss: 2.2412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x10c042160>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=300,\n",
    "          epochs=5,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
